{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOS3t7AbJfO1C0jyW3MRFiW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tesnim19/Ngram/blob/main/FinalNgram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvfk86OeUeYK",
        "outputId": "401f6020-c07e-48bf-9edf-1fe3eed9ac4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the starting word (e.g., he, he was, etc.): i agree with\n",
            "Enter the size of the n-gram (e.g., 2 for bigram, 3 for trigram, etc.): 7\n",
            "\n",
            "Generating a 7-gram sentence starting with 'i agree with': \"i agree with those who said that in my first statement after i testified that it was not contrived enough \"\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import re\n",
        "import random\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "\n",
        "\n",
        "def load_text_from_xml(file_path):\n",
        "    \"\"\"\n",
        "    Load and concatenate text from an XML file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the XML file.\n",
        "\n",
        "    Returns:\n",
        "        str: The concatenated text of all excuses.\n",
        "    \"\"\"\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Extract all the text within <text> tags under each <excuse>\n",
        "    texts = []\n",
        "    for excuse in root.findall('.//excuse'):\n",
        "        text = excuse.find('text').text\n",
        "        if text:\n",
        "            texts.append(text.strip())\n",
        "\n",
        "    # Join all extracted texts into a single string\n",
        "    return ' '.join(texts)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the input text by converting it to lowercase and splitting it into words.\n",
        "\n",
        "    Args:\n",
        "        text (str): The raw text string.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of lowercase words extracted from the text.\n",
        "    \"\"\"\n",
        "    # Use regular expressions to split text into words based on non-alphabetic characters\n",
        "    words = re.split(r'[^A-Za-z]+', text.lower())\n",
        "\n",
        "    # Filter out any empty strings resulting from the split\n",
        "    return list(filter(None, words))\n",
        "\n",
        "def generate_ngram(words, n):\n",
        "    \"\"\"\n",
        "    Generate an n-gram model from a list of words.\n",
        "\n",
        "    Args:\n",
        "        words (list): The list of words from the text.\n",
        "        n (int): The size of the n-gram (e.g., 2 for bigram, 3 for trigram).\n",
        "\n",
        "    Returns:\n",
        "        list: A sorted list of n-grams and their frequencies, sorted by descending frequency.\n",
        "    \"\"\"\n",
        "    gram = {}\n",
        "\n",
        "    # Ensure that n is within a reasonable range\n",
        "    assert 0 < n < 100, \"n must be between 1 and 100\"\n",
        "\n",
        "    # Iterate through the list of words to create n-grams\n",
        "    for i in range(len(words) - (n - 1)):\n",
        "        key = tuple(words[i:i + n])  # Create an n-gram tuple\n",
        "        gram[key] = gram.get(key, 0) + 1  # Increment the frequency count\n",
        "\n",
        "    # Sort the n-grams by frequency in descending order\n",
        "    return sorted(gram.items(), key=lambda item: -item[1])\n",
        "\n",
        "def weighted_choice(choices):\n",
        "    \"\"\"\n",
        "    Make a weighted random choice from a list of (n-gram, frequency) tuples.\n",
        "\n",
        "    Args:\n",
        "        choices (list): A list of tuples where each tuple contains an n-gram and its frequency.\n",
        "\n",
        "    Returns:\n",
        "        tuple: The selected n-gram based on weighted probability.\n",
        "    \"\"\"\n",
        "    # Calculate the total frequency sum\n",
        "    total = sum(freq for _, freq in choices)\n",
        "\n",
        "    # Generate a random number between 0 and the total frequency\n",
        "    r = random.uniform(0, total)\n",
        "    upto = 0\n",
        "\n",
        "    # Iterate through the choices to find where the random number falls\n",
        "    for choice, freq in choices:\n",
        "        if upto + freq > r:\n",
        "            return choice\n",
        "        upto += freq\n",
        "\n",
        "def generate_ngram_sentence(gram, start_words, length=80):\n",
        "    \"\"\"\n",
        "    Generate a sentence using the n-gram model starting with a specific phrase.\n",
        "\n",
        "    Args:\n",
        "        gram (list): The n-gram model as a list of (n-gram, frequency) tuples.\n",
        "        start_words (str): The starting words or phrase.\n",
        "        length (int, optional): The desired length of the generated sentence. Defaults to 50.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated sentence.\n",
        "    \"\"\"\n",
        "    # Split the starting words into a list\n",
        "    start_words = start_words.strip().lower().split()\n",
        "\n",
        "    # to determine the size fo the ngrams\n",
        "    n = len(gram[0][0])\n",
        "\n",
        "    #initializing the sentence\n",
        "    sentence = start_words[:]\n",
        "\n",
        "    for _ in range(length - len(start_words)):\n",
        "        # Get the last (n-1) words from the current sentence as the context\n",
        "        current_context = tuple(sentence[-(n-1):])\n",
        "\n",
        "        # Find all n-grams that match the current context\n",
        "        choices = [element for element in gram if element[0][:len(current_context)] == current_context]\n",
        "\n",
        "        # If no exact match is found, reduce the context size iteratively\n",
        "        while not choices and len(current_context) > 1:\n",
        "            current_context = current_context[1:]\n",
        "            choices = [element for element in gram if element[0][:len(current_context)] == current_context]\n",
        "\n",
        "        # If still no match is found break\n",
        "        if not choices:\n",
        "            break\n",
        "\n",
        "        # Select the next word based on the weighted probability of matching n-grams\n",
        "        next_word = weighted_choice(choices)[len(current_context)]\n",
        "        sentence.append(next_word)\n",
        "\n",
        "    # Combine the list of words into a single string sentence\n",
        "    return ' '.join(sentence)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to interact with the user, generate the n-gram model, and print the generated sentence.\n",
        "    \"\"\"\n",
        "    #the file path to the xml file\n",
        "    file_path = '/content/Excuses.xml'\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "      print(f\"Error: The file '{file_path}' does not exist.\")\n",
        "      return\n",
        "\n",
        "    # Load the text from the XML file\n",
        "    text = load_text_from_xml(file_path)\n",
        "\n",
        "    # Preprocess the text to obtain a list of words\n",
        "    words = preprocess_text(text)\n",
        "\n",
        "    # Prompt the user to enter the starting word for sentence generation\n",
        "    start_word = input(\"Enter the starting word (e.g., he, he was, etc.): \").strip().lower()\n",
        "\n",
        "    # Prompt the user to enter the desired size of the n-gram\n",
        "    try:\n",
        "        ngram_size = int(input(\"Enter the size of the n-gram (e.g., 2 for bigram, 3 for trigram, etc.): \"))\n",
        "        if not (1 < ngram_size < 100):\n",
        "            raise ValueError\n",
        "    except ValueError:\n",
        "        print(\"Invalid n-gram size. Please enter an integer between 2 and 99.\")\n",
        "        return\n",
        "\n",
        "    # Generate the n-gram model based on the specified size\n",
        "    ngram = generate_ngram(words, ngram_size)\n",
        "\n",
        "    # Generate the sentence using the n-gram model\n",
        "    print(f\"\\nGenerating a {ngram_size}-gram sentence starting with '{start_word}': \\\"\", end=\"\")\n",
        "    sentence = generate_ngram_sentence(ngram, start_word, 20)\n",
        "    print(sentence, \"\\\"\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}